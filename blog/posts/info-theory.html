<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>What we borrowed in ML from info theory</title>
    <link rel="stylesheet" href="/css/tufte.css" />
    <link rel="canonical" href="/blog/posts/info-theory.html" />
  </head>
  <body>
    <article class="group">
      <h1>What we borrowed in ML from info theory</h1>
      <p class="subtitle">Notes on entropy, KL, cross-entropy, and mutual information in modern ML.</p>

      <p>
        This post is a short set of notes on how core ideas from information theory shape modern machine learningâ€”loss functions, uncertainty measures, and model evaluation. More details and examples to come.
      </p>

      <h2>Key concepts</h2>
      <ul>
        <li>Entropy as a measure of uncertainty and its role in regularization.</li>
        <li>Cross-entropy and KL divergence as optimization objectives and evaluation metrics.</li>
        <li>Mutual information for feature selection, representation learning, and bottleneck methods.</li>
        <li>Connections to compression, coding, and generalization.</li>
      </ul>

      <p>
        Further sections will include worked examples and links to foundational references.
      </p>
    </article>
  </body>
</html>
