<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>On Expressivity and Simplicity in Model Design</title>
    <meta name="description" content="Why simple baselines remain competitive; notes on inductive bias." />
    <meta property="og:title" content="On Expressivity and Simplicity in Model Design" />
    <meta property="og:description" content="Why simple baselines remain competitive; notes on inductive bias." />
    <meta property="og:type" content="article" />
    <meta property="og:image" content="/assets/profile-placeholder.svg" />
    <link rel="stylesheet" href="/css/tufte.css" />
    <link rel="canonical" href="/blog/posts/first-post.html" />
    <!-- MathJax for mathematical notation rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header>
      <nav class="group">
        <a class="active" href="/">Mohammad ZBeeb</a>
        <span class="spacer"></span>
        <a href="/about/">About</a>
        <a class="btn-lab" href="/lab/">LAIN</a>
        <a class="icon-link" href="https://github.com/Moe-Zbeeb" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
          <svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 .5C5.73.5.5 5.73.5 12c0 5.08 3.29 9.38 7.86 10.9.58.11.79-.25.79-.56 0-.28-.01-1.03-.02-2.02-3.2.69-3.88-1.54-3.88-1.54-.53-1.35-1.3-1.71-1.3-1.71-1.07-.73.08-.72.08-.72 1.18.08 1.8 1.21 1.8 1.21 1.05 1.79 2.75 1.27 3.42.97.11-.76.41-1.27.74-1.56-2.55-.29-5.23-1.28-5.23-5.73 0-1.27.45-2.31 1.2-3.13-.12-.29-.52-1.46.11-3.04 0 0 .98-.31 3.2 1.19.93-.26 1.93-.39 2.92-.39.99 0 1.99.13 2.92.39 2.22-1.5 3.2-1.19 3.2-1.19.63 1.58.23 2.75.11 3.04.75.82 1.2 1.86 1.2 3.13 0 4.46-2.69 5.44-5.25 5.72.42.37.79 1.1.79 2.22 0 1.6-.01 2.88-.01 3.27 0 .31.21.68.8.56A11.53 11.53 0 0 0 23.5 12C23.5 5.73 18.27.5 12 .5z"/></svg>
        </a>
        <a class="icon-link" href="https://scholar.google.com/citations?user=JJg7iTMAAAAJ&hl=en" target="_blank" rel="noopener noreferrer" aria-label="Google Scholar">
          <svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 3L2 9l10 6 10-6-10-6zm0 8.2L5.5 8.5 12 5l6.5 3.5L12 11.2zM5 13v4l7 4 7-4v-4l-7 4-7-4z"/></svg>
        </a>
      </nav>
    </header>

    <article class="post">
      <header>
        <h1>On Speculative Decoding, a Worklog</h1>
        <time datetime="2025-01-01">2025-Nov</time>
      </header>

      <p>
        A simple test-time framework to make inference of large language models faster, taking advantage of the main bottleneck, which is communication hardware, 
        and making use of the unused FLOPs left in the GPU.”
  
      </p>

      <p>
        Two practical reminders: (1) Pay attention to the loss functions presented; (2) Take your time looking the code.
      </p>

      <h3>Motivation</h3> 
      <p>
        Large language models decode tokens token by token in a sequential manner, meaning that this process itself cannot be parallelized. 
        It is also a fact that large language models in deployment are bottlenecked by the communication overhead of moving the weights and the KV cache 
        to and from the disk. <br>
        Given the available FLOPs on certain GPUs at test time, people thought of loading mainly two systems 
        <em>originally another LLM, but this will change as the literature progresses</em> on the GPU. 
        Both are decoding sequentially, but a verification process that should happen at the token level takes place. 
        The main goal is to speed up the output process, so this implies a natural choice of having one of the systems smaller than the other. 
        We generally call the small system the <em>draft</em> and the bigger one the <em>target</em>. 
        <strong>We aim to let the draft model distribution match that of the target one.</strong>
      </p>

      <h3>Core Idea</h3> 
      <div class="post-diagram diagram-pair">
        <img src="/blog/posts/diagrams/2.png" alt="Timeline of draft and target model verification" loading="lazy" />
      </div>
      <p class="image-note"><em>Serving system note:</em> Both the target and draft models live on a single, relaxed serving system, sharing one pool of GPU compute and memory.</p>
      <p>
  <em>Fast Inference from Transformers via Speculative Decoding</em> introduced the idea of speculative decoding, and if someone would love to end the post here, they can read the following:<br>
</p>

<div style="border: 1px solid #ccc; padding: 15px; border-radius: 6px; background-color: #f9f9f9;">
  <p>
    <em>
      Large language model sampling is a sequential process by structure, and it is very slow. Many of the steps in this solution trace are easy, i.e., they can be approximated by a smaller, potentially dumber yet faster model. So, let’s load both models: let the small LLM propose some of the future tokens, and then we can use the bigger one to verify these tokens.
    </em>
  </p>
</div>

<h3>Mindset, Keep the Hardware Warm</h3>
<p>
  There is a notion among people interacting with compute units, for example a GPU, that one of the key responsibilities is to make sure that the full hardware is utilized. In large language model inference systems, the slow part mainly comes from communication with memory: fetching the KV cache, weights, and so on. So there are a lot of FLOPs that can still be utilized by the programmer to make the system more concurrent [1] in nature, such as decoding two LLMs at the same time and verifying their output in parallel.
</p>
<label for="sn-concurrency" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sn-concurrency" class="margin-toggle" />
<span class="sidenote">
 1. Concurrency: doing multiple tasks during overlapping time periods by interleaving work, not necessarily completing them simultaneously.
</span>
<h3>Algorithmic and Mathematical View</h3>
<span class="sidenote">
 2. Fast Inference from Transformers via Speculative Decoding
</span>
<p>
  We denote the <em>target</em> model by
  \(M_p\) and the <em>draft</em> model by
  \(M_q\). Given a prefix \(y\),
  the draft model proposes \(\gamma\) future tokens
  \(x_1, \ldots, x_\gamma\) autoregressively:
</p>

$$q_i(x) = M_q\big(y, x_1, \ldots, x_{i-1}\big), \qquad x_i \sim q_i(x).$$

<p>
  In parallel, the target model evaluates the same sequence of prefixes and
  produces the corresponding conditional probabilities:
</p>

$$p_i(x) = M_p\big(y, x_1, \ldots, x_{i-1}\big),
\qquad i = 1, \ldots, \gamma + 1.$$

<p>
  For each proposed token \(x_i\), we draw
  \(r_i \sim \mathcal{U}(0,1)\) and compare the models'
  likelihoods. The number of accepted tokens \(n\) is:
</p>

$$n = \min\Big(\{\, i - 1 \mid 1 \le i \le \gamma,\
   r_i > \tfrac{p_i(x_i)}{q_i(x_i)} \,\} \cup \{\gamma\}\Big).$$

<p>
  Intuitively, if
  \(\frac{p_i(x_i)}{q_i(x_i)} > 1\),
  the draft model agrees with the target and we accept the token.
  When the ratio is small, we stop trusting the draft model and correct
  the distribution.
</p>

<p>
  After accepting \(n\) draft tokens, the next token is
  drawn from an adjusted distribution:
</p>

$$p'(x) =
\begin{cases}
  p_{n+1}(x), & \text{if } n = \gamma, \\
  \mathrm{norm}\big(\max\{0,\ p_{n+1}(x) - q_{n+1}(x)\}\big),
  & \text{if } n < \gamma.
\end{cases}$$

<p>
  Overall, the sampling law can be written as:
</p>

$$P(x) = \min(P(x), Q(x)) + \big(P(x) - \min(P(x), Q(x))\big),$$

<p>
  ensuring that even after early acceptance or distribution correction,
  the final token is still sampled from the true target distribution
  \(P(x)\).
</p>

<h3>Why This is Faster</h3>
<p>
  1. Batching prefixes plus generated tokens is useful to get answers to many queries at once.
  <br />
  2. Many of the times we accept many tokens.
</p>
<p>
  Note that the target model is able to verify k tokens in parallel.
</p>

<h3>Acceptance and Rejection</h3>
<p>
  We accept a token based on a simple rule: if p(x)/q(x), where p is the probability of the target model and q is the probability
  of the draft model, is bigger than one, we accept the token.
</p>
<p>
  If not, what we are essentially saying is that the probability distribution of the draft model is totally wrong, so using this
  probability just does not make sense now. Doing a sort of distribution correction approach, we sample our next token from a
  corrected distribution, P(x) - Q(x).
</p>

<h3>Checking Our Final Distribution</h3>
<p>
  By construction of our work, we are sampling from:
</p>

$$P(x) = \min(P(x), Q(x)) + \big(P(x) - \min(P(x), Q(x))\big).$$

<p>
  This is saying that we sample from the min of P(x) or Q(x). Preferably, Q(x) is less than P(x) and we end the algorithm here.
  But if it was a bad sequence in time and we happened to find that the models disagree, we correct our distribution P(x) - Q(x).
  Adding the terms up will get us back to P(x), so we are still sampling from the same distribution P(x).
</p>

<p><a href="/">← Back to all posts</a></p>
    </article>
  </body>
  </html>
