<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>On Expressivity and Simplicity in Model Design</title>
    <meta name="description" content="Why simple baselines remain competitive; notes on inductive bias." />
    <meta property="og:title" content="On Expressivity and Simplicity in Model Design" />
    <meta property="og:description" content="Why simple baselines remain competitive; notes on inductive bias." />
    <meta property="og:type" content="article" />
    <meta property="og:image" content="/assets/profile-placeholder.svg" />
    <link rel="stylesheet" href="/css/tufte.css" />
    <link rel="canonical" href="/blog/posts/first-post.html" />
    <!-- MathJax for mathematical notation rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header>
      <nav class="group">
        <a class="active" href="/">Mohammad ZBeeb</a>
        <span class="spacer"></span>
        <a href="/about/">About</a>
        <a class="btn-lab" href="/lab/">LAIN</a>
        <a class="icon-link" href="https://github.com/Moe-Zbeeb" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
          <svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 .5C5.73.5.5 5.73.5 12c0 5.08 3.29 9.38 7.86 10.9.58.11.79-.25.79-.56 0-.28-.01-1.03-.02-2.02-3.2.69-3.88-1.54-3.88-1.54-.53-1.35-1.3-1.71-1.3-1.71-1.07-.73.08-.72.08-.72 1.18.08 1.8 1.21 1.8 1.21 1.05 1.79 2.75 1.27 3.42.97.11-.76.41-1.27.74-1.56-2.55-.29-5.23-1.28-5.23-5.73 0-1.27.45-2.31 1.2-3.13-.12-.29-.52-1.46.11-3.04 0 0 .98-.31 3.2 1.19.93-.26 1.93-.39 2.92-.39.99 0 1.99.13 2.92.39 2.22-1.5 3.2-1.19 3.2-1.19.63 1.58.23 2.75.11 3.04.75.82 1.2 1.86 1.2 3.13 0 4.46-2.69 5.44-5.25 5.72.42.37.79 1.1.79 2.22 0 1.6-.01 2.88-.01 3.27 0 .31.21.68.8.56A11.53 11.53 0 0 0 23.5 12C23.5 5.73 18.27.5 12 .5z"/></svg>
        </a>
        <a class="icon-link" href="https://scholar.google.com/citations?user=JJg7iTMAAAAJ&hl=en" target="_blank" rel="noopener noreferrer" aria-label="Google Scholar">
          <svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 3L2 9l10 6 10-6-10-6zm0 8.2L5.5 8.5 12 5l6.5 3.5L12 11.2zM5 13v4l7 4 7-4v-4l-7 4-7-4z"/></svg>
        </a>
      </nav>
    </header>

    <article class="post">
      <header>
        <h1>On Speculative Decoding, a Worklog</h1>
        <time datetime="2025-01-01">2025-Nov</time>
      </header>

      <p>
        A simple test-time framework to speed up inference for large language models by taking advantage of the main bottleneck—communication hardware—and
        using the unused FLOPs left in the GPU.
      </p>

      <p>
        Two practical reminders: (1) Pay attention to the loss functions presented; (2) Take your time reading the code.
      </p>

      <h3>Motivation</h3> 
      <p>
        Large language models decode tokens one by one in a sequential manner, so this process cannot be parallelized. 
        In deployment, these models are often bottlenecked by the communication overhead of moving the weights and the KV cache 
        to and from disk. <br>
        Given the available FLOPs on certain GPUs at test time, people considered loading two systems 
        <em>originally another LLM, but this will change as the literature progresses</em> onto the GPU. 
        Both decode sequentially, but a token-level verification process runs alongside them. 
        The main goal is to speed up output generation, which naturally suggests making one of the systems smaller than the other. 
        We generally call the small system the <em>draft</em> and the larger one the <em>target</em>. 
        <strong>We aim for the draft model's distribution to match that of the target.</strong>
      </p>

<h3>Core Idea</h3> 
      <div class="post-diagram diagram-pair">
        <img src="/blog/posts/diagrams/2.png" alt="Timeline of draft and target model verification" loading="lazy" />
      </div>
      <p class="image-note"><em>Serving system note:</em> Both the target and draft models live on a single, relaxed serving system, sharing one pool of GPU compute and memory.</p>
      <p>
  <em>Fast Inference from Transformers via Speculative Decoding</em> introduced the idea of speculative decoding, and if someone would like to stop here, they can read the following:<br>
</p>

<div style="border: 1px solid #ccc; padding: 15px; border-radius: 6px; background-color: #f9f9f9;">
  <p>
    <em>
      Large language model sampling is a sequential process by structure, and it is very slow. Many of the steps in this solution trace are easy, i.e., they can be approximated by a smaller, potentially dumber yet faster model. So, let’s load both models: let the small LLM propose some of the future tokens, and then we can use the bigger one to verify these tokens.
    </em>
  </p>
</div>

<h3>Mindset, Keep the Hardware Warm</h3>
<p>
  Practitioners working with compute units—like GPUs—often emphasize fully utilizing the hardware. In large language model inference systems, the slow part mainly comes from communication with memory: fetching the KV cache, weights, and so on. That leaves many FLOPs the programmer can still put to work to make the system more concurrent [1], such as decoding two LLMs at once and verifying their output in parallel.
</p>
<label for="sn-concurrency" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sn-concurrency" class="margin-toggle" />
<span class="sidenote">
 1. Concurrency: doing multiple tasks during overlapping time periods by interleaving work, not necessarily completing them simultaneously.
</span>
<h3>Algorithmic and Mathematical View</h3>

<p>
  We denote the <em>target</em> model by
  \(M_p\) and the <em>draft</em> model by
  \(M_q\). Given a prefix \(y\),
  the draft model proposes \(\gamma\) future tokens
  \(x_1, \ldots, x_\gamma\) autoregressively:
</p>

$$q_i(x) = M_q\big(y, x_1, \ldots, x_{i-1}\big), \qquad x_i \sim q_i(x).$$ 1. 
<span class="sidenote">
 2. Fast Inference from Transformers via Speculative Decoding
</span>

<p>
  In parallel, the target model evaluates the same sequence of prefixes and
  produces the corresponding conditional probabilities:
</p>

$$p_i(x) = M_p\big(y, x_1, \ldots, x_{i-1}\big),
\qquad i = 1, \ldots, \gamma + 1.$$

<p>
  For each proposed token \(x_i\), we draw
  \(r_i \sim \mathcal{U}(0,1)\) and compare the models'
  likelihoods. The number of accepted tokens \(n\) is:
</p>

$$n = \min\Big(\{\, i - 1 \mid 1 \le i \le \gamma,\
   r_i > \tfrac{p_i(x_i)}{q_i(x_i)} \,\} \cup \{\gamma\}\Big).$$

<p>
  Intuitively, if
  \(\frac{p_i(x_i)}{q_i(x_i)} > 1\),
  the draft model agrees with the target and we accept the token.
  When the ratio is small, we stop trusting the draft model and correct
  the distribution.
</p>

<p>
  After accepting \(n\) draft tokens, the next token is
  drawn from an adjusted distribution:
</p>

$$p'(x) =
\begin{cases}
  p_{n+1}(x), & \text{if } n = \gamma, \\
  \mathrm{norm}\big(\max\{0,\ p_{n+1}(x) - q_{n+1}(x)\}\big),
  & \text{if } n < \gamma.
\end{cases}$$

<p>
  Overall, the sampling law can be written as:
</p>

$$P(x) = \min(P(x), Q(x)) + \big(P(x) - \min(P(x), Q(x))\big),$$

<p>
  ensuring that even after early acceptance or distribution correction,
  the final token is still sampled from the true target distribution
  \(P(x)\).
</p>

<h3>Why This is Faster</h3>
<p>
  1. Batching prefixes plus generated tokens is useful to get answers to many queries at once.
  <br />
  2. Often, multiple tokens are accepted at once.
</p>
<p>
  Note that the target model is able to verify k tokens in parallel.
</p>

<h3>Acceptance and Rejection</h3>
<p>
  We accept a token based on a simple rule: if p(x)/q(x) is greater than one—where p is the probability of the target model and q is the probability
  of the draft model—we accept the token.
</p>
<p>
  If not, we are essentially saying the draft model's distribution is off, so using that probability no longer makes sense. Using a distribution-correction approach, we sample the next token from a corrected distribution, P(x) - Q(x).
</p>

<h3>Checking Our Final Distribution</h3>
<p>
  By construction of our work, we are sampling from:
</p>

$$P(x) = \min(P(x), Q(x)) + \big(P(x) - \min(P(x), Q(x))\big).$$

<p>
  This means we sample from the minimum of P(x) and Q(x). Ideally, Q(x) is less than P(x) and we end the algorithm here.
  If the models disagree on a sequence, we correct the distribution with P(x) - Q(x).
  Adding the terms brings us back to P(x), so we are still sampling from the same target distribution.
</p>

<p><a href="/">← Back to all posts</a></p>
    </article>
  </body>
  </html>
