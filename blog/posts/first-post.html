<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>On Expressivity and Simplicity in Model Design</title>
    <meta name="description" content="Why simple baselines remain competitive; notes on inductive bias." />
    <meta property="og:title" content="On Expressivity and Simplicity in Model Design" />
    <meta property="og:description" content="Why simple baselines remain competitive; notes on inductive bias." />
    <meta property="og:type" content="article" />
    <meta property="og:image" content="/assets/profile-placeholder.svg" />
    <link rel="stylesheet" href="/css/tufte.css" />
    <link rel="canonical" href="/blog/posts/first-post.html" />
  </head>
  <body>
    <header>
      <nav class="group">
        <a class="active" href="/">Mohammad ZBeeb</a>
        <span class="spacer"></span>
        <a href="/about/">About</a>
        <a class="btn-lab" href="/lab/">LAIN</a>
        <a class="icon-link" href="https://github.com/Moe-Zbeeb" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
          <svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 .5C5.73.5.5 5.73.5 12c0 5.08 3.29 9.38 7.86 10.9.58.11.79-.25.79-.56 0-.28-.01-1.03-.02-2.02-3.2.69-3.88-1.54-3.88-1.54-.53-1.35-1.3-1.71-1.3-1.71-1.07-.73.08-.72.08-.72 1.18.08 1.8 1.21 1.8 1.21 1.05 1.79 2.75 1.27 3.42.97.11-.76.41-1.27.74-1.56-2.55-.29-5.23-1.28-5.23-5.73 0-1.27.45-2.31 1.2-3.13-.12-.29-.52-1.46.11-3.04 0 0 .98-.31 3.2 1.19.93-.26 1.93-.39 2.92-.39.99 0 1.99.13 2.92.39 2.22-1.5 3.2-1.19 3.2-1.19.63 1.58.23 2.75.11 3.04.75.82 1.2 1.86 1.2 3.13 0 4.46-2.69 5.44-5.25 5.72.42.37.79 1.1.79 2.22 0 1.6-.01 2.88-.01 3.27 0 .31.21.68.8.56A11.53 11.53 0 0 0 23.5 12C23.5 5.73 18.27.5 12 .5z"/></svg>
        </a>
        <a class="icon-link" href="https://scholar.google.com/citations?user=JJg7iTMAAAAJ&hl=en" target="_blank" rel="noopener noreferrer" aria-label="Google Scholar">
          <svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 3L2 9l10 6 10-6-10-6zm0 8.2L5.5 8.5 12 5l6.5 3.5L12 11.2zM5 13v4l7 4 7-4v-4l-7 4-7-4z"/></svg>
        </a>
      </nav>
    </header>

    <article class="post">
      <header>
        <h1>On Speculative Decoding, a Worklog</h1>
        <time datetime="2025-01-01">2025-Nov</time>
      </header>

      <p>
        A simple test-time framework to make inference of large language models faster, taking advantage of the main bottleneck, which is communication hardware, 
        and making use of the unused FLOPs left in the GPU.”
  
      </p>

      <p>
        Two practical reminders: (1) Pay attention to the loss functions presented; (2) Take your time looking the code.
      </p>

      <h3>Motivation</h3> 
      <p>
        Large language models decode tokens token by token in a sequential manner, meaning that this process itself cannot be parallelized. 
        It is also a fact that large language models in deployment are bottlenecked by the communication overhead of moving the weights and the KV cache 
        to and from the disk. <br>
        Given the available FLOPs on certain GPUs at test time, people thought of loading mainly two systems 
        <em>originally another LLM, but this will change as the literature progresses</em> on the GPU. 
        Both are decoding sequentially, but a verification process that should happen at the token level takes place. 
        The main goal is to speed up the output process, so this implies a natural choice of having one of the systems smaller than the other. 
        We generally call the small system the <em>draft</em> and the bigger one the <em>target</em>. 
        <strong>We aim to let the draft model distribution match that of the target one.</strong>
      </p>

      <h3>Core Idea</h3> 
      <div class="post-diagram diagram-pair">
        <img src="/blog/posts/diagrams/2.png" alt="Timeline of draft and target model verification" loading="lazy" />
      </div>
      <p class="image-note"><em>Serving system note:</em> Both the target and draft models live on a single, relaxed serving system, sharing one pool of GPU compute and memory.</p>

      <p><a href="/">← Back to all posts</a></p>
    </article>
  </body>
  </html>
